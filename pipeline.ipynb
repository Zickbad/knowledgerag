{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gKtL4IAh7OZD"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from time import perf_counter as timer\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import display, HTML\n",
    "import ast\n",
    "from itertools import combinations\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gOmqbRdaqB9F"
   },
   "outputs": [],
   "source": [
    "variables = pd.read_csv(\"../program2/variables.csv\")\n",
    "TOPK = 10\n",
    "\n",
    "config_params = ast.literal_eval(variables.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoUC4lMSqB9G",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WFO_JkYOqB9K"
   },
   "outputs": [],
   "source": [
    "rows_dict = {}\n",
    "with open(\"config.txt\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        rows_dict[f\"row_{i+1}\"] = line.strip()\n",
    "#for key, value in rows_dict.items():\n",
    "#    print(f\"{key}: {value}\")\n",
    "llms_conf = rows_dict[\"row_1\"]\n",
    "llms_conf = json.loads(llms_conf)\n",
    "prompts_conf = rows_dict[\"row_2\"]\n",
    "prompts_conf = json.loads(prompts_conf)\n",
    "embeddings_conf = rows_dict[\"row_3\"]\n",
    "embeddings_conf = json.loads(embeddings_conf)\n",
    "temp_conf = rows_dict[\"row_4\"]\n",
    "temp_conf = json.loads(temp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY8p3d8YpDo7"
   },
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=embeddings_conf[config_params[2]], trust_remote_code=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIBvxcch7Rwd"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy.core.multiarray\n",
    "df = pd.read_pickle(\"embeddings_\" + str(config_params[2]) + \".pkl\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pages_and_chunks = df.to_dict(orient=\"records\")\n",
    "embeddings = torch.tensor(np.array(df[\"Embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "#embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2c8ab327a1b44e8186a22516c5f2cfdd",
      "2cc6da1496984809838b1a712b56bdf0",
      "4383ad970a9a49ca938fc11eba093c8e",
      "c83c545daeba47e0a34943cb39b0fe10",
      "e96743e3cedd4dce9cfe5fcf7e342f69",
      "f779a50c90854f948de1503a05472853",
      "1053a893ce3d4255ae9141d99f5ef133",
      "7c540942e9fd49a9a30eee2bc221a5e8",
      "2a26000fbf6f4cd893ebfd30adcc44b0",
      "f7d5a0df49ca490baf9bdf1af37e67ad",
      "668fa258fa954666a059da672c11dc48"
     ]
    },
    "editable": true,
    "id": "YSnwhZdGfiXQ",
    "outputId": "fff46371-077e-436d-edd4-9217f6a37995",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(llms_conf[config_params[0]],\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         low_cpu_mem_usage=True)\n",
    "llm_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llms_conf[config_params[0]])\n",
    "tokenizer.pad_token_id=tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dVKl0L98Ap9"
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    context = \"- \" + \"\\n- \".join([item[\"Abstract\"] for item in context_items])\n",
    "    base_prompt = prompts_conf[config_params[1]][0]\n",
    "    #print(base_prompt)\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "    #print(base_prompt)\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FlILVVp7m1j"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=TOPK,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    start_time = timer()\n",
    "    if config_params[2]==0:\n",
    "        dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    else:\n",
    "        dot_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvAPDPAU8Ckb"
   },
   "outputs": [],
   "source": [
    "def ask(query,\n",
    "        temperature=temp_conf[config_params[3]],\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True,\n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings)\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu()\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 max_new_tokens=max_new_tokens,\n",
    "                                 do_sample=True)\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcAuS6fi7eN_"
   },
   "outputs": [],
   "source": [
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubh3xJKG8EVv",
    "outputId": "8d36b14c-e88e-49d0-96ba-524ac15d1f61",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#query = \"What are tailings, and what are the challenges associated with their management in the mineral processing industry?\"\n",
    "query = \"What are tailings, and how do environmental, chemical, and geotechnical factors influence sustainable tailings management in mineral processing operations?\"\n",
    "print(f\"Query: {query}\")\n",
    "answer, context_items = ask(query, return_answer_only=False)\n",
    "#print(f\"Answer:\\n\")\n",
    "#print_wrapped(answer)\n",
    "#print(f\"Context items:\")\n",
    "#context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGGi5bBlqB9Q"
   },
   "source": [
    "# Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Yv9F0CTqB9Q",
    "outputId": "f85ddb45-6bc9-4832-942e-7d06bf51f0af"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of texts extracted: {len(context_items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ci6ky7TqB9R",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a cosine similarity function optimized for PyTorch tensors\n",
    "def cosine_similarity_pytorch(matrix):\n",
    "    \"\"\"\n",
    "    Computes pairwise cosine similarity for a given matrix of vectors.\n",
    "\n",
    "    Args:\n",
    "        matrix (torch.Tensor): A 2D tensor where each row is a vector.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 2D tensor containing cosine similarity scores.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of each vector with all others\n",
    "    dot_products = torch.mm(matrix, matrix.T)\n",
    "    # Compute the L2 norm for each vector\n",
    "    norms = torch.norm(matrix, dim=1, keepdim=True)\n",
    "    # Compute the outer product of norms\n",
    "    norm_products = torch.mm(norms, norms.T)\n",
    "    # Return cosine similarity scores\n",
    "    return dot_products / norm_products\n",
    "\n",
    "pd.options.display.float_format = '{:.6f}'.format  # Display values with six decimal places\n",
    "\n",
    "# Step 1: Extract abstracts from the provided list\n",
    "abstracts_from_list = [item['Abstract'] for item in context_items]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#similar_abstracts = []\n",
    "\n",
    "\"\"\"\n",
    "for idx, list_abstract in enumerate(abstracts_from_list):\n",
    "    # Crear un corpus con el abstract actual y los del DataFrame\n",
    "    corpus = [list_abstract] + df['Abstract'].tolist()\n",
    "\n",
    "    # Transformar el corpus en una matriz TF-IDF dispersa\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Mantener la matriz como dispersa\n",
    "    tfidf_tensor_sparse = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "\n",
    "    # Normalizar los vectores\n",
    "    norms = torch.norm(tfidf_tensor_sparse, dim=1, keepdim=True)\n",
    "    normalized_tensors = tfidf_tensor_sparse / norms\n",
    "\n",
    "    # Calcular similitudes coseno usando matrices densas en la CPU\n",
    "    similarity_matrix = torch.mm(normalized_tensors, normalized_tensors.T)\n",
    "\n",
    "    # Extraer similitudes para el vector base\n",
    "    base_similarities = similarity_matrix[0, 1:]\n",
    "    similar_indices = torch.where(base_similarities > 0.95)[0]\n",
    "\n",
    "    for i in similar_indices.numpy():\n",
    "        similar_abstracts.append({\n",
    "            \"Title\": f\"Text{idx + 1}\",\n",
    "            \"Abstract\": df.iloc[i]['Abstract'],\n",
    "            \"Similarity\": base_similarities[i].item()\n",
    "        })\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "result_df = pd.DataFrame(abstracts_from_list).rename(columns={0: \"Abstract\"})\n",
    "result_df[\"Title\"] = pd.Index(range(TOPK))\n",
    "result_df = result_df[[\"Title\", \"Abstract\"]]\n",
    "\n",
    "# Display the results\n",
    "#result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MK7abNNqB9R"
   },
   "outputs": [],
   "source": [
    "if (config_params[0] == 0) :\n",
    "    # Usar una expresión regular para extraer el contenido entre las etiquetas\n",
    "    #match = re.search(r\"<\\|begin_of_text\\|>(.*?)<\\|eot_id\\|>\", answer, re.DOTALL)\n",
    "    new_text = answer.replace(\"<|begin_of_text|>\", \"\").strip()\n",
    "    new_text = answer.replace(\"<|im_end|>\", \"\").strip()\n",
    "    #print(match)\n",
    "    #if match:\n",
    "    #    new_text = match.group(1).strip()\n",
    "    #    #print(new_text)\n",
    "    #else:\n",
    "    #    new_text = \"\"\n",
    "    #    print(\"No se encontró texto entre las etiquetas.\")\n",
    "\n",
    "elif (config_params[0] == 1) :\n",
    "    new_text = answer.replace(\"<|im_end|>\", \"\").strip()\n",
    "\n",
    "elif (config_params[0] == 2) :\n",
    "    new_text = answer.strip()\n",
    "\n",
    "elif (config_params[0] == 3) :\n",
    "    new_text = answer.replace(\"<|im_end|>\", \"\").strip()\n",
    "\n",
    "else:\n",
    "    new_text = \"\"\n",
    "\n",
    "new_title = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpk78AYRqB9R",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing: considers the whole text as one block, ignores sentence boundaries\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the whole text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    return filtered_tokens  # Return a flat list of filtered tokens\n",
    "\n",
    "# Build co-occurrence graph for the whole text (no sentence boundary)\n",
    "def build_cooccurrence_network(tokens, window_size=3):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Slide window and create edges for each token pair\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i + window_size]\n",
    "        for w1, w2 in combinations(window, 2):\n",
    "            if G.has_edge(w1, w2):\n",
    "                G[w1][w2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(w1, w2, weight=1)\n",
    "    return G\n",
    "\n",
    "# Process abstracts from the DataFrame\n",
    "networks = []\n",
    "for abstract in result_df[\"Abstract\"]:\n",
    "    tokens = preprocess(abstract)\n",
    "    G = build_cooccurrence_network(tokens)\n",
    "    networks.append(G)\n",
    "\n",
    "# Process additional text\n",
    "tokens_extra = preprocess(new_text)\n",
    "G_extra = build_cooccurrence_network(tokens_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2DlJSvoqB9R"
   },
   "outputs": [],
   "source": [
    "def Natural_connectivity(graph):#https://arxiv.org/pdf/2105.00419    http://dx.doi.org/10.1016/j.physa.2017.01.0720\n",
    "    L  = nx.adjacency_spectrum(graph)\n",
    "    L1 = sum(np.exp(np.real(L[i])) for i in range(graph.number_of_nodes()))\n",
    "    return np.log(L1/graph.number_of_nodes())/(graph.number_of_nodes() - np.log(graph.number_of_nodes()))\n",
    "\n",
    "def Conductance(graph): #hhttp://dx.doi.org/10.1016/j.physa.2017.01.072\n",
    "    L     = nx.laplacian_matrix(graph)\n",
    "    eig   = np.round(np.linalg.eigvals(L.toarray()),5)\n",
    "    N     = graph.number_of_nodes()\n",
    "    S     = 0\n",
    "    for i in range(len(eig)):\n",
    "        if np.real(eig[i])!=0:\n",
    "            S = S + 1/np.real(eig[i])\n",
    "    return (N-1)/(N*S)\n",
    "\n",
    "def SpectralRadius(G):# doi:10.1016/j.disc.2019.05.017\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "    A    = nx.adjacency_matrix(G).toarray()\n",
    "    eig  = np.linalg.eigvalsh(A)\n",
    "    spec = np.max(np.abs(eig))\n",
    "    k = None\n",
    "    for pot_k in range(3, n + 1):\n",
    "        lb = (pot_k - 2) * (pot_k - 3) / 2\n",
    "        ub = pot_k * (pot_k - 3) / 2\n",
    "        if lb <= m - n <= ub:\n",
    "            k = pot_k\n",
    "            break\n",
    "    if k is None:\n",
    "        return spec / (n - 1)\n",
    "    t1 = 2 * m - n - k + 2.5\n",
    "    t2 = np.sqrt(2 * m - 2 * n + 2.25)\n",
    "    upper_bound = np.sqrt(t1 + t2)\n",
    "    normalized_radius = spec / upper_bound if upper_bound != 0 else 0.0\n",
    "    return normalized_radius\n",
    "\n",
    "# Revisar\n",
    "def SpectralGap(graph):\n",
    "    L     = nx.normalized_laplacian_matrix(graph).toarray()\n",
    "    eig   = np.linalg.eigvalsh(L)\n",
    "    eig.sort()\n",
    "    return np.real(eig[1])/2\n",
    "\n",
    "def AverageDegree(graph):\n",
    "    N = nx.number_of_nodes(graph.copy())\n",
    "    return (sum(dict(graph.degree()).values())/N)/(N-1)\n",
    "\n",
    "def AverageEdgeBetweenness(graph): #vulnerability of complex networks Igor mishkovski\n",
    "    N  = nx.number_of_nodes(graph.copy())\n",
    "    return (1/nx.average_shortest_path_length(graph) - 1)/(6/(N*(N+1)) - 1)\n",
    "\n",
    "def calculate_modularity(graph):\n",
    "    partition = community.best_partition(graph)\n",
    "    modularity = community.modularity(partition, graph)\n",
    "    return modularity\n",
    "\n",
    "def calculate_metrics(graph):\n",
    "    metrics = {}\n",
    "    metrics['Connected Components'] = nx.number_connected_components(graph)\n",
    "    if not nx.is_connected(graph):\n",
    "        graph = graph.subgraph(max(nx.connected_components(graph), key=len)).copy()\n",
    "    # Connectivity\n",
    "    metrics['Global Efficiency'] = nx.global_efficiency(graph) # max\n",
    "    metrics['Average Edge Betweenness'] = AverageEdgeBetweenness(graph)# max\n",
    "    # Adjacency Matrix Spectrum\n",
    "    metrics['Spectral Radius'] = SpectralRadius(graph) # max\n",
    "    metrics['Spectral Gap'] = SpectralGap(graph) # max\n",
    "    metrics['Natural Connectivity'] = Natural_connectivity(graph) # max\n",
    "    # Laplacian Matrix Spectrum\n",
    "    metrics['Algebra Connectivity'] = nx.algebraic_connectivity(graph)/graph.number_of_nodes() #max\n",
    "    metrics['Conductance'] = Conductance(graph) # max\n",
    "\n",
    "    # Null Model\n",
    "    nullModel = nx.Graph(nx.configuration_model([deg for node, deg in graph.degree()]))\n",
    "    metrics['G Modularity'] = calculate_modularity(graph)\n",
    "    metrics['Null Model Modularity'] = calculate_modularity(nullModel)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCi8R-f3qB9S"
   },
   "outputs": [],
   "source": [
    "# Function to plot the interactive graph and save it as an HTML file\n",
    "def plot_interactive_graph_colab(graph, filename=\"graph.html\"):\n",
    "    # Compute node positions using NetworkX\n",
    "    pos = nx.spring_layout(graph)  # You can replace this with another layout like nx.circular_layout(graph)\n",
    "\n",
    "    # Create PyVis graph\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\", cdn_resources=\"in_line\")\n",
    "\n",
    "    # Add nodes with precomputed positions\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        x, y = pos[node]  # Get the position of the node\n",
    "        net.add_node(\n",
    "            node,\n",
    "            title=f\"Word: {node}\",\n",
    "            color=data.get('color', 'blue'),  # Default to blue if no color is specified\n",
    "            x=x * 1000,  # Scale positions for PyVis\n",
    "            y=y * 1000,\n",
    "            label=\"\"  # Hide labels if desired\n",
    "        )\n",
    "\n",
    "    # Add edges\n",
    "    for source, target in graph.edges():\n",
    "        net.add_edge(source, target)\n",
    "\n",
    "    # Disable physics\n",
    "    net.set_options(\"\"\"\n",
    "    {\n",
    "        \"physics\": {\n",
    "            \"enabled\": false\n",
    "        }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    filename = f\"Graphs/{filename}\"\n",
    "\n",
    "    # Save the HTML file\n",
    "    net.save_graph(filename)\n",
    "\n",
    "    # Display the graph directly in the notebook or environment\n",
    "    with open(filename, \"r\") as f:\n",
    "        display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOwKjABcqB9S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "\n",
    "# Display the graphs of the 10 abstracts and calculate metrics\n",
    "for idx, (title, graph) in enumerate(zip(result_df[\"Title\"], networks)):\n",
    "    # Create a valid filename (without special characters)\n",
    "    safe_title = \"\".join(c if c.isalnum() else \"_\" for c in str(title))\n",
    "    filename = f\"Abstract_{idx+1}_{safe_title}.html\"\n",
    "\n",
    "    # Display and save graph\n",
    "    # plot_interactive_graph_colab(graph, filename)\n",
    "\n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        m = calculate_metrics(graph)\n",
    "        m[\"Title\"] = title\n",
    "        m['config'] = config_params\n",
    "        m['i'] = variables.iloc[0,1]\n",
    "    except Exception as e:\n",
    "        print(\"Error 1\")\n",
    "        m = {\"Title\": title, \"Error\": str(e)}\n",
    "    metrics_list.append(m)\n",
    "\n",
    "# Graph and metrics for the additional abstract\n",
    "# plot_interactive_graph_colab(G_extra, \"New_Text_Extra.html\")\n",
    "\n",
    "try:\n",
    "    m_extra = calculate_metrics(G_extra)\n",
    "    m_extra[\"Title\"] = \"Proposed\"\n",
    "    m_extra['config'] = config_params\n",
    "    m_extra['i'] = variables.iloc[0,1]\n",
    "except Exception as e:\n",
    "    print(\"Error 2\")\n",
    "    m_extra = {\"Title\": \"Proposed\", \"Error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all metrics\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter valid metrics (exclude errors and null values)\n",
    "valid_metrics_df = metrics_df.dropna(subset=[\n",
    "    col for col in metrics_df.columns \n",
    "    if col not in [\"Title\", \"Error\"]\n",
    "])\n",
    "\n",
    "# Compute the average of each metric, excluding the 'Title' column\n",
    "average_metrics = valid_metrics_df.drop(columns=[\"Title\", \"config\", \"i\", \"Connected Components\", \"G Modularity\", \"Null Model Modularity\"]).mean()\n",
    "\n",
    "# Convert the m_extra dictionary to a Series to ensure consistent ordering\n",
    "m_extra_series = pd.Series({k: v for k, v in m_extra.items() if k in average_metrics.index})\n",
    "\n",
    "# Reorder m_extra_series to match average_metrics\n",
    "m_extra_series = m_extra_series[average_metrics.index]\n",
    "\n",
    "# Normalize each metric\n",
    "normalized_avg = []\n",
    "normalized_extra = []\n",
    "\n",
    "for metric in average_metrics.index:\n",
    "    avg_val = average_metrics[metric]\n",
    "    extra_val = m_extra_series[metric]\n",
    "    max_val = max(avg_val, extra_val)\n",
    "\n",
    "    if max_val > 0:\n",
    "        normalized_avg.append(avg_val / max_val)\n",
    "        normalized_extra.append(extra_val / max_val)\n",
    "    else:\n",
    "        normalized_avg.append(0)\n",
    "        normalized_extra.append(0)\n",
    "\n",
    "# Close the polygon\n",
    "normalized_avg += normalized_avg[:1]\n",
    "normalized_extra += normalized_extra[:1]\n",
    "angles = np.linspace(0, 2 * np.pi, len(average_metrics) + 1, endpoint=True)\n",
    "\n",
    "# Create radar chart\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, normalized_avg, label='Average Abstracts', linestyle='solid')\n",
    "ax.fill(angles, normalized_avg, alpha=0.25)\n",
    "\n",
    "ax.plot(angles, normalized_extra, label='New Text', linestyle='dashed')\n",
    "ax.fill(angles, normalized_extra, alpha=0.25)\n",
    "\n",
    "# Labels\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), list(average_metrics.index))\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "plt.title(\"Radar of Normalized Metrics: Average vs New Text\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "def calculate_radar_area(values, angles):\n",
    "    n = len(values)\n",
    "    area = 0.5 * np.abs(\n",
    "        sum(values[i] * values[i + 1] * np.sin(angles[i + 1] - angles[i]) for i in range(n - 1))\n",
    "    )\n",
    "    return area\n",
    "\n",
    "# Area calculation\n",
    "area_avg = calculate_radar_area(normalized_avg, angles)\n",
    "area_new = calculate_radar_area(normalized_extra, angles)\n",
    "\n",
    "print(\"Area Before:\", area_avg)\n",
    "print(\"Area After:\", area_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../program2/resultadoExperimento.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame([{\n",
    "    'config' : config_params,\n",
    "    'i' : variables.iloc[0,1],\n",
    "    'n_abstracts' : abstracts_from_list,\n",
    "    'proposed' : new_text,\n",
    "    'metrics_avg' : average_metrics.to_list(),\n",
    "    'metrics_new' : m_extra_series.to_list(),\n",
    "    'delta_metrics' : (m_extra_series - average_metrics).to_list(),\n",
    "    'area_avg' : area_avg,\n",
    "    'area_new' : area_new\n",
    "}])\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_path):\n",
    "    bd = pd.read_csv(file_path)\n",
    "    bd = pd.concat([bd, result], ignore_index=True)\n",
    "else:\n",
    "    bd = result  # Si no existe, creamos un nuevo DataFrame con los datos\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "bd.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con número de nodos y aristas por grafo\n",
    "graph_stats = []\n",
    "for idx, (title, graph) in enumerate(zip(result_df[\"Title\"], networks)):\n",
    "    graph_stats.append({\n",
    "        \"config\": config_params,\n",
    "        \"i\": variables.iloc[0,1],\n",
    "        \"Title\": title,\n",
    "        \"Num_Nodes\": graph.number_of_nodes(),\n",
    "        \"Num_Edges\": graph.number_of_edges()\n",
    "    })\n",
    "\n",
    "graph_stats.append({\n",
    "        \"config\": config_params,\n",
    "        \"i\": variables.iloc[0,1],\n",
    "        \"Title\": \"Proposed\",\n",
    "        \"Num_Nodes\": G_extra.number_of_nodes(),\n",
    "        \"Num_Edges\": G_extra.number_of_edges()\n",
    "    })\n",
    "graph_stats_df = pd.DataFrame(graph_stats)\n",
    "\n",
    "# Crear DataFrame con el degree de cada palabra por grafo\n",
    "degree_data = []\n",
    "for idx, (title, graph) in enumerate(zip(result_df[\"Title\"], networks)):\n",
    "    for node, degree in graph.degree():\n",
    "        degree_data.append({\n",
    "            \"config\": config_params,\n",
    "            \"i\": variables.iloc[0,1],\n",
    "            \"Title\": title,\n",
    "            \"Word\": node,\n",
    "            \"Degree\": degree\n",
    "        })\n",
    "\n",
    "for node, degree in G_extra.degree():\n",
    "    degree_data.append({\n",
    "        \"config\": config_params,\n",
    "        \"i\": variables.iloc[0,1],\n",
    "        \"Title\": \"Proposed\",\n",
    "        \"Word\": node,\n",
    "        \"Degree\": degree\n",
    "    })\n",
    "\n",
    "degree_df = pd.DataFrame(degree_data)\n",
    "\n",
    "# Generar una lista de DataFrames con las matrices de adyacencia\n",
    "adjacency_matrices = {}\n",
    "for idx, (title, graph) in enumerate(zip(result_df[\"Title\"], networks)):\n",
    "    matrix = nx.to_pandas_adjacency(graph, dtype=int)\n",
    "    adjacency_matrices[title] = matrix  # guardar usando el título como clave\n",
    "\n",
    "matrix = nx.to_pandas_adjacency(G_extra, dtype=int)\n",
    "adjacency_matrices[\"Proposed\"] = matrix  # guardar usando el título como clave\n",
    "\n",
    "# Mostrar resultados\n",
    "#graph_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../program2/ExtraData/resultadoGraphStats.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    bd = pd.read_csv(file_path)\n",
    "    bd = pd.concat([bd, graph_stats_df], ignore_index=True)\n",
    "else:\n",
    "    bd = graph_stats_df  # Si no existe, creamos un nuevo DataFrame con los datos\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "bd.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_df[degree_df[\"Title\"] == 0].sort_values(\"Degree\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../program2/ExtraData/resultadoGraphDegree.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    bd = pd.read_csv(file_path)\n",
    "    bd = pd.concat([bd, degree_df], ignore_index=True)\n",
    "else:\n",
    "    bd = degree_df  # Si no existe, creamos un nuevo DataFrame con los datos\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "bd.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjacency_matrices[\"Proposed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_data = []\n",
    "\n",
    "for title, matrix in adjacency_matrices.items():\n",
    "    stacked = matrix.stack().reset_index()\n",
    "    stacked.columns = ['source', 'target', 'value']\n",
    "    for _, row in stacked.iterrows():\n",
    "        adjacency_data.append({\n",
    "            \"config\": config_params,\n",
    "            \"i\": variables.iloc[0, 1],\n",
    "            \"Title\": title,\n",
    "            \"source\": row['source'],\n",
    "            \"target\": row['target'],\n",
    "            \"value\": row['value']\n",
    "        })\n",
    "\n",
    "adjacency_df = pd.DataFrame(adjacency_data)\n",
    "\n",
    "adjacency_df = adjacency_df[adjacency_df[\"value\"] != 0].reset_index(drop=True)\n",
    "\n",
    "file_path = \"../program2/ExtraData/resultadoAdjacencyMatrix.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    bd = pd.read_csv(file_path)\n",
    "    bd = pd.concat([bd, adjacency_df], ignore_index=True)\n",
    "else:\n",
    "    bd = adjacency_df  # Si no existe, creamos un nuevo DataFrame con los datos\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "bd.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list.append(m_extra)\n",
    "# Create DataFrame with all metrics\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "#metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../program2/ExtraData/resultadoMetrics.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    bd = pd.read_csv(file_path)\n",
    "    bd = pd.concat([bd, metrics_df], ignore_index=True)\n",
    "else:\n",
    "    bd = metrics_df  # Si no existe, creamos un nuevo DataFrame con los datos\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "bd.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1053a893ce3d4255ae9141d99f5ef133": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a26000fbf6f4cd893ebfd30adcc44b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c8ab327a1b44e8186a22516c5f2cfdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cc6da1496984809838b1a712b56bdf0",
       "IPY_MODEL_4383ad970a9a49ca938fc11eba093c8e",
       "IPY_MODEL_c83c545daeba47e0a34943cb39b0fe10"
      ],
      "layout": "IPY_MODEL_e96743e3cedd4dce9cfe5fcf7e342f69"
     }
    },
    "2cc6da1496984809838b1a712b56bdf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f779a50c90854f948de1503a05472853",
      "placeholder": "​",
      "style": "IPY_MODEL_1053a893ce3d4255ae9141d99f5ef133",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "4383ad970a9a49ca938fc11eba093c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c540942e9fd49a9a30eee2bc221a5e8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a26000fbf6f4cd893ebfd30adcc44b0",
      "value": 2
     }
    },
    "668fa258fa954666a059da672c11dc48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c540942e9fd49a9a30eee2bc221a5e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c83c545daeba47e0a34943cb39b0fe10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7d5a0df49ca490baf9bdf1af37e67ad",
      "placeholder": "​",
      "style": "IPY_MODEL_668fa258fa954666a059da672c11dc48",
      "value": " 2/2 [00:45&lt;00:00, 20.54s/it]"
     }
    },
    "e96743e3cedd4dce9cfe5fcf7e342f69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f779a50c90854f948de1503a05472853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7d5a0df49ca490baf9bdf1af37e67ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
